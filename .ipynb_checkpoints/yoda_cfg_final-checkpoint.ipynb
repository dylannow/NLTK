{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67978301",
   "metadata": {},
   "source": [
    "Analyzing and importing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156f34c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Most frequent words\n",
      "1             (., 154)\n",
      "2             (,, 127)\n",
      "3            (the, 64)\n",
      "4            (you, 54)\n",
      "5             (to, 33)\n",
      "6             (is, 31)\n",
      "7              (i, 27)\n",
      "8              (a, 22)\n",
      "9           (will, 20)\n",
      "10            (of, 17)\n",
      "11          (have, 16)\n",
      "12            (we, 15)\n",
      "13             (?, 15)\n",
      "14           (not, 15)\n",
      "15            (be, 14)\n",
      "16          (your, 14)\n",
      "17          (this, 13)\n",
      "18          (must, 13)\n",
      "19          (with, 13)\n",
      "20         (force, 13)\n",
      "Length of the text in characters: 6005\n",
      "Amount of sentences in corpus: 171\n",
      "Number of unique words: 405\n",
      "Avarage sentence length: 34.10526315789474\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize corpus\n",
    "raw = open(\"yoda.txt\").read()\n",
    "corpus = raw.lower()\n",
    "corpus = corpus.replace(\"\\n\", \" \")\n",
    "\n",
    "# Length of corpus\n",
    "length_corpus = len(corpus)\n",
    "\n",
    "# Length of sentences\n",
    "sentences = sent_tokenize(corpus)\n",
    "amount_sentences = len(sentences)\n",
    "\n",
    "# Number of unique words\n",
    "vocab_punc = sorted(set(word_tokenize(corpus)))\n",
    "words = [w.lower() for w in vocab_punc]\n",
    "hapaxes = len(words)\n",
    "\n",
    "# Average sentence length in characters\n",
    "total_characters = 0\n",
    "for i in sentences:\n",
    "    total_characters += len(i)\n",
    "average_sentence_length = total_characters / len(sentences)\n",
    "\n",
    "# Most frequent words\n",
    "freq_dist = nltk.FreqDist(word_tokenize(corpus))\n",
    "most_frequent_words = freq_dist.most_common(20)\n",
    "\n",
    "# Visualize the data\n",
    "data = {\"Most frequent words\": most_frequent_words}\n",
    "df = pd.DataFrame(data)\n",
    "df.index = pd.RangeIndex(start=1, stop=len(df) + 1)\n",
    "print(df)\n",
    "print(f\"Length of the text in characters: {length_corpus}\")\n",
    "print(f\"Amount of sentences in corpus: {amount_sentences}\")\n",
    "print(f\"Number of unique words: {hapaxes}\")\n",
    "print(f\"Avarage sentence length: {average_sentence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc0ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
