{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1105f8",
   "metadata": {},
   "source": [
    "### Importing modules and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6798598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import CFG\n",
    "from nltk.parse import ChartParser\n",
    "\n",
    "import pandas as pd\n",
    "import string \n",
    "\n",
    "# Initialize corpus\n",
    "raw = open(\"yoda.txt\").read()\n",
    "corpus = raw.lower()\n",
    "corpus = corpus.replace(\"\\n\", \" \")\n",
    "\n",
    "#Download POS tagger\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6ba0a",
   "metadata": {},
   "source": [
    "### Analyzing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f887e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Most frequent words\n",
      "1            (the, 64)\n",
      "2            (you, 54)\n",
      "3             (to, 33)\n",
      "4             (is, 31)\n",
      "5              (i, 27)\n",
      "6              (a, 22)\n",
      "7           (will, 20)\n",
      "8             (of, 17)\n",
      "9           (have, 16)\n",
      "10            (we, 15)\n",
      "11           (not, 15)\n",
      "12            (be, 14)\n",
      "13          (your, 14)\n",
      "14          (this, 13)\n",
      "15          (must, 13)\n",
      "16          (with, 13)\n",
      "17         (force, 13)\n",
      "18           (are, 11)\n",
      "19            (in, 11)\n",
      "20           (and, 11)\n",
      "Length of the text in characters: 6005\n",
      "Amount of sentences in corpus: 171\n",
      "Number of unique words: 397\n",
      "Avarage sentence length: 35.11695906432749\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(corpus)\n",
    "words = nltk.word_tokenize(corpus)\n",
    "\n",
    "# Remove punctuation\n",
    "words = [i.lower() for i in words if i.isalpha()]\n",
    "\n",
    "# Calculate corpus data\n",
    "length_corpus = len(corpus)\n",
    "amount_sentences = len(sentences)\n",
    "hapaxes = len(set(words))\n",
    "average_sentence_length = length_corpus / amount_sentences\n",
    "most_frequent_words = nltk.FreqDist(words).most_common(20)\n",
    "\n",
    "# Visualize the data\n",
    "data = {\"Most frequent words\": most_frequent_words}\n",
    "df = pd.DataFrame(data)\n",
    "df.index = pd.RangeIndex(start=1, stop=len(df) + 1)\n",
    "print(df)\n",
    "print(f\"Length of the text in characters: {length_corpus}\")\n",
    "print(f\"Amount of sentences in corpus: {amount_sentences}\")\n",
    "print(f\"Number of unique words: {hapaxes}\")\n",
    "print(f\"Avarage sentence length: {average_sentence_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa6aaf",
   "metadata": {},
   "source": [
    "### Constructing CFG\n",
    "\n",
    "1. Object-Subject-Verb Structure:\n",
    "   - \"Done, it is.\"\n",
    "   - \"A Jedi's strength flows from the Force.\"\n",
    "2. Interrogative Sentences:\n",
    "   - \"More to learn, he has?\"\n",
    "   - \"What know you of ready?\"\n",
    "3. Idiomatic Phrases:\n",
    "   - \"May the Force be with you.\"\n",
    "   - \"Fear is the path to the dark side.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ea491f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/home/jesse/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5875/1840805796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtagged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Define CFG rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             )\n\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/home/jesse/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Master Qui-Gon, more to say have you?\",\n",
    "    \"A vergence, you say?\",\n",
    "    \"An apprentice, you have, Qui-Gon.\",\n",
    "    \"Master Kenobi, dark times are these.\",\n",
    "    \"Master Kenobi, our spies contact, you must.\",\n",
    "    \"Master Kenobi, my choice is.\",\n",
    "    \"Your father he is.\",\n",
    "    \"May the Force be with you.\",\n",
    "    \"Fear is the path to the dark side.\",\n",
    "    \"Death is a natural part of life.\",\n",
    "    \"Train yourself to let go.\",\n",
    "    \"In a dark place we find ourselves.\",\n",
    "    \"Only pain will you find.\",\n",
    "    \"At an end your rule is.\",\n",
    "    \"Failed to stop the Sith Lord, I have.\",\n",
    "    \"To become one with the Force.\",\n",
    "    \"To his family, send him.\",\n",
    "    \"Done, it is.\",\n",
    "    \"Soon will I rest.\",\n",
    "    \"No more training do you require.\",\n",
    "    \"A Jedi's strength flows from the Force.\",\n",
    "    \"Revealed your opinion is.\",\n",
    "    \"Good, young one. How feel you?\",\n",
    "    \"Afraid are you?\",\n",
    "    \"See through you, we can?\",\n",
    "    \"Ready are you?\",\n",
    "    \"More to learn, he has?\",\n",
    "    \"What know you of ready?\",\n",
    "    \"Told you, did he?\",\n",
    "    \"Unexpected this is.\"\n",
    "]\n",
    "\n",
    "# Tag words in corpus\n",
    "tagged = []\n",
    "\n",
    "for i in sentences:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tagged.extend(nltk.pos_tag(words))\n",
    "\n",
    "# Define CFG rules\n",
    "# grammar = CFG.fromstring(\"\"\"\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f0d39",
   "metadata": {},
   "source": [
    "### Parsing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15e7f095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing: ['Master', 'Qui-Gon', ',', 'more', 'to', 'say', 'have', 'you', '?']\n",
      "Error parsing: ['A', 'vergence', ',', 'you', 'say', '?']\n",
      "Error parsing: ['An', 'apprentice', ',', 'you', 'have', ',', 'Qui-Gon', '.']\n",
      "Error parsing: ['Master', 'Kenobi', ',', 'dark', 'times', 'are', 'these', '.']\n",
      "Error parsing: ['Master', 'Kenobi', ',', 'our', 'spies', 'contact', ',', 'you', 'must', '.']\n",
      "Error parsing: ['Master', 'Kenobi', ',', 'my', 'choice', 'is', '.']\n",
      "Error parsing: ['Your', 'father', 'he', 'is', '.']\n",
      "Error parsing: ['Fear', 'is', 'the', 'path', 'to', 'the', 'dark', 'side', '.']\n",
      "Error parsing: ['Death', 'is', 'a', 'natural', 'part', 'of', 'life', '.']\n",
      "Error parsing: ['Train', 'yourself', 'to', 'let', 'go', '.']\n",
      "Error parsing: ['In', 'a', 'dark', 'place', 'we', 'find', 'ourselves', '.']\n",
      "Error parsing: ['Only', 'pain', 'will', 'you', 'find', '.']\n",
      "Error parsing: ['At', 'an', 'end', 'your', 'rule', 'is', '.']\n",
      "Error parsing: ['Failed', 'to', 'stop', 'the', 'Sith', 'Lord', ',', 'I', 'have', '.']\n",
      "Error parsing: ['To', 'become', 'one', 'with', 'the', 'Force', '.']\n",
      "Error parsing: ['To', 'his', 'family', ',', 'send', 'him', '.']\n",
      "Error parsing: ['Done', ',', 'it', 'is', '.']\n",
      "Error parsing: ['Soon', 'will', 'I', 'rest', '.']\n",
      "Error parsing: ['No', 'more', 'training', 'do', 'you', 'require', '.']\n",
      "Error parsing: ['A', 'Jedi', \"'s\", 'strength', 'flows', 'from', 'the', 'Force', '.']\n",
      "Error parsing: ['Revealed', 'your', 'opinion', 'is', '.']\n",
      "Error parsing: ['Good', ',', 'young', 'one', '.', 'How', 'feel', 'you', '?']\n",
      "Error parsing: ['Afraid', 'are', 'you', '?']\n",
      "Error parsing: ['See', 'through', 'you', ',', 'we', 'can', '?']\n",
      "Error parsing: ['Ready', 'are', 'you', '?']\n",
      "Error parsing: ['More', 'to', 'learn', ',', 'he', 'has', '?']\n",
      "Error parsing: ['What', 'know', 'you', 'of', 'ready', '?']\n",
      "Error parsing: ['Told', 'you', ',', 'did', 'he', '?']\n",
      "Error parsing: ['Unexpected', 'this', 'is', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(i) for i in sentences]\n",
    "\n",
    "#Parse each sentence according to CFG\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "for i in tokenized_sentences:\n",
    "    try:\n",
    "        for tree in parser.parse(i):\n",
    "            print(tree)\n",
    "    except ValueError:\n",
    "        print(f'Error parsing: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93890ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
