{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e5e21e0",
   "metadata": {},
   "source": [
    "### Importing modules and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912aafce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jesse/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import CFG\n",
    "from nltk.parse import ChartParser\n",
    "\n",
    "import pandas as pd\n",
    "import string \n",
    "\n",
    "# Initialize corpus\n",
    "raw = open(\"yoda.txt\").read()\n",
    "corpus = raw.lower()\n",
    "corpus = corpus.replace(\"\\n\", \" \")\n",
    "\n",
    "#Download POS tagger\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d63b771e",
   "metadata": {},
   "source": [
    "### Analyzing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6af929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Most frequent words\n",
      "1            (the, 64)\n",
      "2            (you, 54)\n",
      "3             (to, 33)\n",
      "4             (is, 31)\n",
      "5              (i, 27)\n",
      "6              (a, 22)\n",
      "7           (will, 20)\n",
      "8             (of, 17)\n",
      "9           (have, 16)\n",
      "10            (we, 15)\n",
      "11           (not, 15)\n",
      "12            (be, 14)\n",
      "13          (your, 14)\n",
      "14          (this, 13)\n",
      "15          (must, 13)\n",
      "16          (with, 13)\n",
      "17         (force, 13)\n",
      "18           (are, 11)\n",
      "19            (in, 11)\n",
      "20           (and, 11)\n",
      "Length of the text in characters: 6005\n",
      "Amount of sentences in corpus: 171\n",
      "Number of unique words: 397\n",
      "Avarage sentence length: 35.11695906432749\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(corpus)\n",
    "words = nltk.word_tokenize(corpus)\n",
    "\n",
    "# Remove punctuation\n",
    "words = [i.lower() for i in words if i.isalpha()]\n",
    "\n",
    "# Calculate corpus data\n",
    "length_corpus = len(corpus)\n",
    "amount_sentences = len(sentences)\n",
    "hapaxes = len(set(words))\n",
    "average_sentence_length = length_corpus / amount_sentences\n",
    "most_frequent_words = nltk.FreqDist(words).most_common(20)\n",
    "\n",
    "# Visualize the data\n",
    "data = {\"Most frequent words\": most_frequent_words}\n",
    "df = pd.DataFrame(data)\n",
    "df.index = pd.RangeIndex(start=1, stop=len(df) + 1)\n",
    "print(df)\n",
    "print(f\"Length of the text in characters: {length_corpus}\")\n",
    "print(f\"Amount of sentences in corpus: {amount_sentences}\")\n",
    "print(f\"Number of unique words: {hapaxes}\")\n",
    "print(f\"Avarage sentence length: {average_sentence_length}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ed9413a",
   "metadata": {},
   "source": [
    "### Constructing CFG\n",
    "\n",
    "1. Object-Subject-Verb Structure:\n",
    "   - \"Done, it is.\"\n",
    "   - \"A Jedi's strength flows from the Force.\"\n",
    "2. Interrogative Sentences:\n",
    "   - \"More to learn, he has?\"\n",
    "   - \"What know you of ready?\"\n",
    "3. Idiomatic Phrases:\n",
    "   - \"May the Force be with you.\"\n",
    "   - \"Fear is the path to the dark side.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca0375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Master Qui-Gon, more to say have you?\",\n",
    "    \"A vergence, you say?\",\n",
    "    \"An apprentice, you have, Qui-Gon.\",\n",
    "    \"Master Kenobi, dark times are these.\",\n",
    "    \"Master Kenobi, our spies contact, you must.\",\n",
    "    \"Master Kenobi, my choice is.\",\n",
    "    \"Your father he is.\",\n",
    "    \"May the Force be with you.\",\n",
    "    \"Fear is the path to the dark side.\",\n",
    "    \"Death is a natural part of life.\",\n",
    "    \"Train yourself to let go.\",\n",
    "    \"In a dark place we find ourselves.\",\n",
    "    \"Only pain will you find.\",\n",
    "    \"At an end your rule is.\",\n",
    "    \"Failed to stop the Sith Lord, I have.\",\n",
    "    \"To become one with the Force.\",\n",
    "    \"To his family, send him.\",\n",
    "    \"Done, it is.\",\n",
    "    \"Soon will I rest.\",\n",
    "    \"No more training do you require.\",\n",
    "    \"A Jedi's strength flows from the Force.\",\n",
    "    \"Revealed your opinion is.\",\n",
    "    \"Good, young one. How feel you?\",\n",
    "    \"Afraid are you?\",\n",
    "    \"See through you, we can?\",\n",
    "    \"Ready are you?\",\n",
    "    \"More to learn, he has?\",\n",
    "    \"What know you of ready?\",\n",
    "    \"Told you, did he?\",\n",
    "    \"Unexpected this is.\"\n",
    "]\n",
    "        \n",
    "\n",
    "# Tag words in sentences\n",
    "tagged = []\n",
    "\n",
    "for i in sentences:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    tagged.extend(nltk.pos_tag(words))\n",
    "    \n",
    "# Generate the lexical rules file\n",
    "with open(\"lex_rules.txt\", \"w\") as file:\n",
    "    for word, pos in tagged:\n",
    "        file.write(f\"{pos} -> '{word}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cfda554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining CFG \n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    JJR -> 'Master' | 'more'\n",
    "    NNP -> 'Qui-Gon' | 'Master' | 'Kenobi' | 'May' | 'Sith' | 'Lord' | 'Afraid' | 'Told'\n",
    "    DT -> 'A' | 'An' | 'the' | 'your' | 'No'\n",
    "    NN -> 'vergence' | 'apprentice' | 'times' | 'Master' | 'choice' | 'father' | 'path' | 'side' | 'part' | 'life' | 'place' | 'pain' | 'end' | 'rule' | 'training' | 'Jedi' | 'strength' | 'opinion' | 'one' | 'Force' | 'training'\n",
    "    PRP -> 'you' | 'he' | 'we' | 'I' | 'my' | 'him' | 'we'\n",
    "    VBP -> 'say' | 'have' | 'are' | 'contact' | 'find' | 'require' | 'know' | 'are'\n",
    "    VB -> 'say' | 'have' | 'be' | 'Train' | 'let' | 'go' | 'find' | 'stop' | 'become' | 'send' | 'reveal' | 'see' | 'feel' | 'learn'\n",
    "    IN -> 'to' | 'of' | 'with' | 'in' | 'from' | 'through'\n",
    "    RB -> 'Only' | 'Soon'\n",
    "    CD -> 'one'\n",
    "    WRB -> 'How'\n",
    "\n",
    "    \n",
    "    \"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "204c9040",
   "metadata": {},
   "source": [
    "### Parsing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9b947d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing: ['Master', 'Qui-Gon', ',', 'more', 'to', 'say', 'have', 'you', '?']\n",
      "Error parsing: ['A', 'vergence', ',', 'you', 'say', '?']\n",
      "Error parsing: ['An', 'apprentice', ',', 'you', 'have', ',', 'Qui-Gon', '.']\n",
      "Error parsing: ['Master', 'Kenobi', ',', 'dark', 'times', 'are', 'these', '.']\n",
      "Error parsing: ['Master', 'Kenobi', ',', 'our', 'spies', 'contact', ',', 'you', 'must', '.']\n",
      "Error parsing: ['Master', 'Kenobi', ',', 'my', 'choice', 'is', '.']\n",
      "Error parsing: ['Your', 'father', 'he', 'is', '.']\n",
      "Error parsing: ['May', 'the', 'Force', 'be', 'with', 'you', '.']\n",
      "Error parsing: ['Fear', 'is', 'the', 'path', 'to', 'the', 'dark', 'side', '.']\n",
      "Error parsing: ['Death', 'is', 'a', 'natural', 'part', 'of', 'life', '.']\n",
      "Error parsing: ['Train', 'yourself', 'to', 'let', 'go', '.']\n",
      "Error parsing: ['In', 'a', 'dark', 'place', 'we', 'find', 'ourselves', '.']\n",
      "Error parsing: ['Only', 'pain', 'will', 'you', 'find', '.']\n",
      "Error parsing: ['At', 'an', 'end', 'your', 'rule', 'is', '.']\n",
      "Error parsing: ['Failed', 'to', 'stop', 'the', 'Sith', 'Lord', ',', 'I', 'have', '.']\n",
      "Error parsing: ['To', 'become', 'one', 'with', 'the', 'Force', '.']\n",
      "Error parsing: ['To', 'his', 'family', ',', 'send', 'him', '.']\n",
      "Error parsing: ['Done', ',', 'it', 'is', '.']\n",
      "Error parsing: ['Soon', 'will', 'I', 'rest', '.']\n",
      "Error parsing: ['No', 'more', 'training', 'do', 'you', 'require', '.']\n",
      "Error parsing: ['A', 'Jedi', \"'s\", 'strength', 'flows', 'from', 'the', 'Force', '.']\n",
      "Error parsing: ['Revealed', 'your', 'opinion', 'is', '.']\n",
      "Error parsing: ['Good', ',', 'young', 'one', '.', 'How', 'feel', 'you', '?']\n",
      "Error parsing: ['Afraid', 'are', 'you', '?']\n",
      "Error parsing: ['See', 'through', 'you', ',', 'we', 'can', '?']\n",
      "Error parsing: ['Ready', 'are', 'you', '?']\n",
      "Error parsing: ['More', 'to', 'learn', ',', 'he', 'has', '?']\n",
      "Error parsing: ['What', 'know', 'you', 'of', 'ready', '?']\n",
      "Error parsing: ['Told', 'you', ',', 'did', 'he', '?']\n",
      "Error parsing: ['Unexpected', 'this', 'is', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(i) for i in sentences]\n",
    "\n",
    "#Parse each sentence according to CFG\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "for i in tokenized_sentences:\n",
    "    try:\n",
    "        for tree in parser.parse(i):\n",
    "            print(tree)\n",
    "    except ValueError:\n",
    "        print(f'Error parsing: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba80b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
